# SALM model configuration for Canary-Qwen-2.5B ASR
# Based on speechlm2 SALM recipe with lhotse shar dataset format

name: "SALM-Canary-Qwen-2.5B"

model:
  # Training log prediction
  log_prediction_train: true
  log_prediction_train_samples: 5
  log_prediction_train_interval: 50
  # Validation log prediction
  log_prediction_valid: true
  log_prediction_valid_samples: 20

  # Pre-trained model settings
  pretrained_llm: /path/to/Qwen3-1.7B/
  pretrained_asr: /path/to/pretrained_asr.nemo
  pretrained_weights: True  # Use pretrained weights (False for random init)

  # Freeze parameters - ONLY freeze LLM, keep encoder and projection trainable
  freeze_params:
    - "^llm\\..+$"  # Freeze entire LLM
    - "^embed_tokens\\..+$"
  prevent_freeze_params:
    - ".*lora_.*"  # Keep all LoRA parameters trainable

  # Prompt configuration
  prompt_format: canary_qwen  # Use canary_qwen format for ASR with Qwen model
  audio_locator_tag: "<|audio|>"  # Placeholder for audio in text

  # LoRA configuration
  lora:
    task_type: CAUSAL_LM
    r: 128  # LoRA rank
    lora_alpha: 256  # LoRA alpha scaling
    lora_dropout: 0.01
    target_modules: ["q_proj", "v_proj"]
    inference_mode: false  # Training mode

  # Audio perception module configuration
  perception:
    target: nemo.collections.speechlm2.modules.perception.AudioPerceptionModule
    output_dim: 2048
    modality_adapter:
      _target_: nemo.collections.speechlm2.modules.perception.IdentityConnector
      d_model: 1024

  # Optimizer configuration
  optimizer:
    _target_: torch.optim.AdamW
    lr: 5e-4
    betas: [0.9, 0.98]
    weight_decay: 1e-3
    foreach: true  # Efficient optimizer update

  # Learning rate scheduler
  lr_scheduler:
    _target_: nemo.core.optim.lr_scheduler.InverseSquareRootAnnealing
    warmup_steps: 1000
    warmup_ratio: null
    min_lr: 1e-6
    max_steps: 100000

# Trainer configuration
trainer:
  devices: -1  # Auto-detect number of GPUs (use all available)
  accelerator: gpu
  num_nodes: -1  # Auto-detect number of nodes (via MPI)
  precision: bf16-true  # Mixed precision training

  # Training configuration - Official: 90k steps on 32 GPUs
  max_steps: 400000 # 100000*4=400000 # Official training steps
  max_epochs: 1
  # limit_train_batches: 3200  # Steps per "epoch"
  val_check_interval: 400  # Validate every 400 steps
  limit_val_batches: 64

  # Logging and checkpointing
  log_every_n_steps: 8
  num_sanity_val_steps: 1
  gradient_clip_val: 1.0
  accumulate_grad_batches: 4

  # Strategy for distributed training
  strategy:
    _target_: lightning.pytorch.strategies.DDPStrategy
    gradient_as_bucket_view: true
    find_unused_parameters: true

  # Disable defaults (provided by exp_manager)
  logger: False
  enable_checkpointing: False
  use_distributed_sampler: False

# Data configuration with lhotse shar format
data:
  train_ds:
    sample_rate: 16000
    prompt_format: ${model.prompt_format}
    token_equivalent_duration: 0.08  # Audio frame to token conversion ratio

    # ASR task context prompt - MATCHES OFFICIAL CANARY-QWEN-2.5B TRAINING
    asr_context_prompt: "Transcribe the following: "

    # Dataset configuration using lhotse_as_conversation for SALM
    # This automatically converts ASR-only shar datasets to conversational format (auto-converted to input_cfg by speech_to_text_salm.py)
    use_lhotse: true
    shar_path:
      - [/path/to/dataset1, weight1]
      - [/path/to/dataset2, weight2]

    # Batching configuration
    use_multimodal_sampling: true
    min_duration: 0.3
    max_duration: 40.0  # Official maximum audio duration
    min_tokens: 2
    max_tokens: 1024
    bucket_duration_bins: [99, 110, 117, 124, 184, 247, 324, 391, 457, 520, 555, 579, 600, 618, 638, 1024]
    bucket_batch_size: [69, 64, 60, 40, 28, 22, 16, 14, 12, 11, 10, 6, 4, 4, 4, 2]
    use_bucketing: true
    num_buckets: 16
    bucket_buffer_size: 20000

    # Data loading configuration
    seed: 42
    shuffle: true
    shard_seed: "randomized"
    num_workers: 8

  validation_ds:
    prompt_format: ${model.prompt_format}  # Will resolve to canary_qwen
    token_equivalent_duration: 0.08
    sample_rate: 16000

    # ASR context prompt - this will be used as default context
    asr_context_prompt: "Transcribe the following: "

    datasets:
      validation_set1:
        manifest_filepath: /path/to/validation_set1.json
        tags:
          context: ${data.validation_ds.asr_context_prompt}
      validation_set2:
        manifest_filepath: /path/to/validation_set2.json
        tags:
          context: ${data.validation_ds.asr_context_prompt}

    batch_size: 1
    seed: 42
    shard_seed: "randomized"
    num_workers: 8
    shuffle: false

# Experiment manager configuration
exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true

  # Resume training configuration
  resume_from_checkpoint: null
  resume_if_exists: true
  resume_ignore_no_checkpoint: true

  # Checkpoint configuration
  checkpoint_callback_params:
    monitor: val_wer
    mode: min
    save_top_k: 20
    filename: ${name}-{${exp_manager.checkpoint_callback_params.monitor}:.4f}-{step}
    always_save_nemo: true
    save_nemo_on_train_end: true

  # Weights & Biases logging (optional)
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: ${name}
    project: salm-canary-qwen
    resume: true