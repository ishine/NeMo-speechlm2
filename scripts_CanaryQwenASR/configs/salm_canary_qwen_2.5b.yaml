# SALM model configuration for Canary-Qwen-2.5B ASR
# Based on speechlm2 SALM recipe with lhotse shar dataset format

name: "SALM-Canary-Qwen-2.5B"

model:
  # Pretrained models for initialization
  # Option 1: Use HuggingFace model ID (downloads automatically)
  # pretrained_llm: Qwen/Qwen3-1.7B

  # Option 2: Use locally downloaded model (faster startup, no internet required)
  pretrained_llm: /path/to/Qwen3-1.7B/  # Local path to Qwen3-1.7B

  # ASR encoder - Using local Canary model
  pretrained_asr: /path/to/encoder.nemo  # Custom encoder (requires compatibility shim)
  # pretrained_asr: /path/to/canary-1b-v2/canary-1b-v2.nemo  # Using official Canary encoder (downloaded on local machine)

  pretrained_weights: True  # Use pretrained weights (False for random init)

  # Freeze parameters - ONLY freeze LLM, keep encoder and projection trainable
  freeze_params:
    # Frozen LLM only (as per official training)
    - "^llm\\..+$"  # Freeze entire LLM
    - "^embed_tokens\\..+$"  # Freeze LLM embeddings
  prevent_freeze_params:
    # Ensure LoRA parameters are trainable even if they match LLM pattern
    - ".*lora_.*"  # Keep all LoRA parameters trainable

  # Prompt configuration
  prompt_format: canary_qwen  # Use canary_qwen format for ASR with Qwen model
  audio_locator_tag: "<|audio|>"  # Placeholder for audio in text

  # LoRA configuration - REQUIRED as per official training
  lora:
    task_type: CAUSAL_LM
    r: 16  # LoRA rank
    lora_alpha: 32  # LoRA alpha scaling
    lora_dropout: 0.1
    target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    inference_mode: false  # Training mode

  # Audio perception module configuration
  perception:
    target: nemo.collections.speechlm2.modules.perception.AudioPerceptionModule
    modality_adapter:
      _target_: nemo.collections.asr.modules.ConformerEncoder
      feat_in: 1024  # Match Canary encoder output dimension
      feat_out: -1   # Use d_model as output
      n_layers: 4    # Number of adapter layers (increased for better adaptation)
      d_model: 1024  # Hidden dimension

      # Subsampling configuration
      subsampling: dw_striding  # Depth-wise striding for efficiency
      subsampling_factor: 1     # No additional subsampling (Canary already subsamples)
      subsampling_conv_channels: 256
      causal_downsampling: false

      # Feed-forward configuration
      ff_expansion_factor: 4

      # Self-attention configuration
      self_attention_model: rel_pos  # Relative positional encoding
      n_heads: 8
      att_context_size: [-1, -1]  # Unlimited context
      att_context_style: regular
      xscaling: true  # Scale embeddings by sqrt(d_model)
      untie_biases: true
      pos_emb_max_len: 5000

      # Convolution configuration
      conv_kernel_size: 9
      conv_norm_type: batch_norm
      conv_context_size: null  # Centered context

      # Regularization
      dropout: 0.1
      dropout_pre_encoder: 0.1
      dropout_emb: 0.0
      dropout_att: 0.1

  # Optimizer configuration
  optimizer:
    _target_: torch.optim.AdamW
    lr: 3e-4  # Learning rate for adapter layers
    betas: [0.9, 0.98]
    weight_decay: 1e-3
    foreach: true  # Efficient optimizer update

  # Learning rate scheduler
  lr_scheduler:
    _target_: nemo.core.optim.lr_scheduler.CosineAnnealing
    warmup_steps: 2000  # Warmup period
    min_lr: 1e-6
    max_steps: ${trainer.max_steps}

# Trainer configuration
trainer:
  devices: -1  # Auto-detect number of GPUs (use all available)
  accelerator: gpu
  num_nodes: -1  # Auto-detect number of nodes (via MPI)
  precision: bf16-true  # Mixed precision training

  # Training configuration - Official: 90k steps on 32 GPUs
  max_steps: 90000  # Official training steps
  limit_train_batches: 2000  # Steps per "epoch"
  val_check_interval: 1000  # Validate every 1000 steps
  limit_val_batches: 50

  # Gradient accumulation for effective batch size increase
  # With batch_tokens=3500 and accumulate_grad_batches=1, effective batch = 3500 tokens
  # No additional accumulation needed since batch_tokens is already large
  accumulate_grad_batches: 1

  # Logging and checkpointing
  log_every_n_steps: 10
  num_sanity_val_steps: 2
  gradient_clip_val: 1.0

  # Strategy for distributed training
  strategy:
    # Option 1: Standard DDP (default, works for most cases)
    _target_: lightning.pytorch.strategies.DDPStrategy
    gradient_as_bucket_view: true
    find_unused_parameters: true

    # Option 2: FSDP for larger models or memory constraints
    # _target_: nemo.lightning.pytorch.strategies.FSDPStrategy
    # auto_wrap_policy: {nemo.collections.speechlm2.modules.TransformerLayer}
    # state_dict_type: "sharded"
    # sharding_strategy: "FULL_SHARD"  # or "SHARD_GRAD_OP" for gradient sharding only

    # Option 3: ModelParallelStrategy for tensor/pipeline parallelism
    # _target_: lightning.pytorch.strategies.ModelParallelStrategy
    # tensor_parallel_size: 1
    # pipeline_parallel_size: 1

  # Disable defaults (provided by exp_manager)
  logger: False
  enable_checkpointing: False
  use_distributed_sampler: False

# Data configuration with lhotse shar format
data:
  train_ds:
    sample_rate: 16000
    prompt_format: ${model.prompt_format}
    token_equivalent_duration: 0.08  # Audio frame to token conversion ratio

    # ASR task context prompt - MATCHES OFFICIAL CANARY-QWEN-2.5B TRAINING
    # Official training used: "Transcribe the following: " (with colon and space)
    # Reference: https://huggingface.co/nvidia/canary-qwen-2.5b
    asr_context_prompt: "Transcribe the following: "

    # Dataset configuration using lhotse_as_conversation for SALM
    # This automatically converts ASR-only shar datasets to conversational format
    use_lhotse: true

    # Option 1: Using shar_path (auto-converted to input_cfg by speech_to_text_salm.py)
    # This is maintained for backward compatibility with existing scripts
    # Note: Comment out datasets that don't exist or use correct paths
    shar_path: [
      [/path/to/lhotse/shar/dataset1, 40810515],
      [/path/to/lhotse/shar/dataset2, 94878730],
      [/path/to/lhotse/shar/dataset3, 28124100],
    ]

    # Option 2: Using input_cfg directly (preferred for SALM)
    # Uncomment below and comment out shar_path above to use this format
    # input_cfg:
    #   - type: lhotse_as_conversation
    #     shar_path: /path/to/lhotse/shar/dataset1
    #     audio_locator_tag: ${model.audio_locator_tag}
    #     token_equivalent_duration: ${data.train_ds.token_equivalent_duration}
    #     tags:
    #       context: ${data.train_ds.asr_context_prompt}
    #       # system_prompt: "You are an expert speech transcriber."  # Optional

    # Batching configuration
    batch_size: null  # Use bucketing instead
    use_bucketing: true
    use_multimodal_sampling: true
    measure_total_length: true

    # Token-based batching - Optimized for A100 80GB GPU memory utilization
    batch_tokens: 5500  # Tokens per GPU
    max_tokens: 4096  # Maximum sequence length (increased to accommodate longer audio sequences)

    # Bucket configuration - 16 buckets for fine-grained batching efficiency
    # Granular buckets ensure optimal packing across diverse audio lengths (0.3-40s range)
    bucket_duration_bins: [64, 128, 192, 256, 384, 512, 640, 768, 1024, 1280, 1536, 2048, 2560, 3072, 3584, 4096]
    num_buckets: 16
    bucket_buffer_size: 20000 

    # Data loading configuration
    seed: 42
    shuffle: true
    shard_seed: "randomized"
    num_workers: 8

    # Duration filtering - Official: max 40 seconds
    min_duration: 0.3
    max_duration: 40.0  # Official maximum audio duration

  validation_ds:
    prompt_format: ${model.prompt_format}  # Will resolve to canary_qwen
    token_equivalent_duration: 0.08
    sample_rate: 16000

    # ASR context prompt - this will be used as default context
    asr_context_prompt: "Transcribe the following: "

    datasets:
      librispeech_test_clean:
        manifest_filepath: /path/to/LibriSpeech/test_clean.json
        # Add context configuration for validation
        tags:
          context: ${data.validation_ds.asr_context_prompt}
      librispeech_test_other:
        manifest_filepath: /path/to/LibriSpeech/test_other.json
        tags:
          context: ${data.validation_ds.asr_context_prompt}
      librispeech_dev_clean:
        manifest_filepath: /path/to/LibriSpeech/dev_clean.json
        tags:
          context: ${data.validation_ds.asr_context_prompt}
      librispeech_dev_other:
        manifest_filepath: /path/to/LibriSpeech/dev_other.json
        tags:
          context: ${data.validation_ds.asr_context_prompt}

    batch_size: 4
    seed: 42
    shard_seed: "randomized"
    num_workers: 8
    shuffle: false

# Experiment manager configuration
exp_manager:
  exp_dir: null
  explicit_log_dir: scripts_CanaryQwenASR/outputs
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true

  # Resume training configuration
  resume_from_checkpoint: null
  resume_if_exists: true
  resume_ignore_no_checkpoint: true

  # Checkpoint configuration
  checkpoint_callback_params:
    filename: "{step}-{val_loss:.3f}"
    monitor: val_loss
    mode: min
    # every_n_train_steps: 2000
    save_top_k: 5
    always_save_nemo: false
    save_nemo_on_train_end: true

  # Weights & Biases logging (optional)
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: ${name}
    project: salm-canary-qwen
    resume: true